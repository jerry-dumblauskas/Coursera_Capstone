---
title: "Exploratory Analysis to build a word prediction function"
author: "Jerry Dumblauskas"
date: "February 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction and Goals

This is an analysis report for the Coursera Data Science Capstone project. The goal of this analysis is to come up with some ideas for a predictive text model using a large text repository of documents as training data. Basic R techniques will be used to perform the analysis and build the groundwork for an eventual predictive model.

This report describes the major features of the training data with our exploratory data analysis and summarizes plans for creating the predictive model.


## Analysis
### 1) Get the Data
Here is code for getting the data.  This set is huge, so don't run it unless you mean it!  Change the XXX to d396qusza40orc if you want to run it.

```{r}
my_file<-"Coursera-SwiftKey.zip"
if (!file.exists(my_file)) {
  download.file("https://XXX.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile=my_file)
  unzip(my_file)
}
```

This file has data in 4 languages: English, French, Russian, and German.  We will extract and use the english data for this analysis.
```{r}
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
news <- readLines("final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul = TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
```

These are hefty files.  Take a look:
```{r}
object.size(blogs)
object.size(news)
object.size(twitter)
```

We are not going to worry about the size that much.  We will use bigger machines/processing here to get the job done.

### 2) Clean the Data
Here, we will use the tm package as per the class.  Documentation is at: <https://cran.r-project.org/web/packages/tm/tm.pdf>
We'll take 1 percent of the data as a sample.  We also need to strip out emojis! (thanks stackoverflow)


```{r}
library(tm)

set.seed(4322)
sample_data <- c(sample(blogs, length(blogs) * 0.01), sample(news, length(news) * 0.01), sample(twitter, length(twitter) * 0.01))

# Create corpora and clean the data
corpora <- VCorpus(VectorSource(sample_data))
to_space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpora <- tm_map(corpora, to_space, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpora <- tm_map(corpora, to_space, "@[^\\s]+")
corpora <- tm_map(corpora, function(x) iconv(enc2utf8(x$content), sub = "byte"))
corpora <- tm_map(corpora, tolower)
corpora <- tm_map(corpora, removeWords, stopwords("en"))
corpora <- tm_map(corpora, removeNumbers)
corpora <- tm_map(corpora, removePunctuation)
corpora <- tm_map(corpora, stripWhitespace)
corpora <- tm_map(corpora, PlainTextDocument)
```


### 3) Analyze the Data
As a first step, let's look at 3, 2 and 1 level 'grams' AKA trigrams, bigrams and unigrams.  From the class notes we'll use the Rweka package to help (and of course ggplot2)

```{r}
library(ggplot2)
library(RWeka)
```

now let's create the plot data:
```{r}

# Top N variables
datapoints <- 40

get_freq <- function(tdm) 
{
  freq <- sort(rowSums (as.matrix(tdm)), decreasing = TRUE)
  return(data.frame(word = names(freq), freq = freq))
}

bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

make_plot <- function(data, label) 
{
  ggplot(data[1:datapoints,], aes(reorder(word, -freq), freq)) +
    labs(x = label, y = "Count") +
    theme(axis.text.x = element_text(angle = 75, hjust = 1, size = 14)) +
    geom_bar(stat = "identity", fill = I("grey30"))
}

# Get freqs (uni, bi and tri)
freq1 <- get_freq(removeSparseTerms(TermDocumentMatrix(corpora), 0.999))
freq2 <- get_freq(removeSparseTerms(TermDocumentMatrix(corpora, control = list(tokenize = bigram)), 0.999))
freq3 <- get_freq(removeSparseTerms(TermDocumentMatrix(corpora, control = list(tokenize = trigram)), 0.9999))
```

and let's look at the graphs
```{r}
make_plot(freq3, paste(datapoints ,"Top Triples in the Sample set"))
make_plot(freq2, paste(datapoints ,"Top Doubles in the Sample set"))
make_plot(freq1, paste(datapoints ,"Top Singles in the Sample set"))
```


## Observations and Next Steps

This is a start on creating a predictive algorithm.  At this point I am not worried about processing power.  Going to up to 3 previous words doesn't stress a standard workstation heavily, in fact we can experiment with going higher.

We'll want to build this set of Ngrams and create function that can be called by a client, which will pass in a string.  We'll parse this string and send back a result based on the greatest number of tokenized strings passed in.  Our trick will be to make this responsive to a calling client.
