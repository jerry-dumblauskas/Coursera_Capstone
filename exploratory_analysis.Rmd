---
title: "Exploratory Analysis"
author: "Jerry Dumblauskas"
date: "February 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction and Goals

This is an analysis report for the Coursera Data Science Capstone project. The goal of this analysis is to create a predictive text model using a large text repository of documents as training data. Basic R techniques will be used to perform the analysis and build the predictive model.

This report describes the major features of the training data with our exploratory data analysis and summarizes plans for creating the predictive model.


## Analysis
### 1) Get the Data
Here is code for getting the data.  This set is huge, so don't run it unless you mean it!  Change the XXX to d396qusza40orc if you want to run

```{r}
if (!file.exists("Coursera-SwiftKey.zip")) {
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile="Coursera-SwiftKey.zip")
  unzip("Coursera-SwiftKey.zip")
}
```

This file has data in 4 languages: English, French, Russian, and German.  We will extract and use the english data for this analysis.
```{r}
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
news <- readLines("final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul = TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
```

These are hefty files.  Take a look:
```{r}
object.size(blogs)
object.size(news)
object.size(twitter)
```

Not going to try and cut down on the size that much.  We will use bigger machines/processing here to get the job done.

### 2) Clean the Data
Here, we will use the tm package as per the class.  Documentation is at: <https://cran.r-project.org/web/packages/tm/tm.pdf>
We'll take 1 percent of the data as a sample.  We also need to strip out emojis! (thanks stackoverflow)


```{r}
library(tm)

set.seed(4322)
data.sample <- c(sample(blogs, length(blogs) * 0.01),
                 sample(news, length(news) * 0.01),
                 sample(twitter, length(twitter) * 0.01))

# Create corpora and clean the data
corpora <- VCorpus(VectorSource(data.sample))
to_space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpora <- tm_map(corpora, to_space, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpora <- tm_map(corpora, to_space, "@[^\\s]+")
corpora <- tm_map(corpora, function(x) iconv(enc2utf8(x$content), sub = "byte"))
corpora <- tm_map(corpora, tolower)
corpora <- tm_map(corpora, removeWords, stopwords("en"))
corpora <- tm_map(corpora, removePunctuation)
corpora <- tm_map(corpora, removeNumbers)
corpora <- tm_map(corpora, stripWhitespace)
corpora <- tm_map(corpora, PlainTextDocument)
```


### 3) Analyze the Data
As a first step, let's look at 3, 2 and 1 level 'grams' AKA trigrams, bigrams and unigrams.  From the class notes we'll use the Rweka package to help (and of course ggplot2)

```{r}
library(RWeka)
library(ggplot2)
```

now let's create the plot data:
```{r}

datapoints <- 40

get_freq <- function(tdm) {
  freq <- sort(rowSums (as.matrix(tdm)), decreasing = TRUE)
  return(data.frame(word = names(freq), freq = freq))
}
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
makePlot <- function(data, label) {
  ggplot(data[1:datapoints,], aes(reorder(word, -freq), freq)) +
    labs(x = label, y = "Count") +
    theme(axis.text.x = element_text(angle = 60, size = 12, hjust = 1)) +
    geom_bar(stat = "identity", fill = I("grey30"))
}

# Get freqs (uni, bi and tri)
freq1 <- get_freq(removeSparseTerms(TermDocumentMatrix(corpora), 0.9999))
freq2 <- get_freq(removeSparseTerms(TermDocumentMatrix(corpora, control = list(tokenize = bigram)), 0.9999))
freq3 <- get_freq(removeSparseTerms(TermDocumentMatrix(corpora, control = list(tokenize = trigram)), 0.9999))
```

and let's look at the graphs
```{r}
makePlot(freq3, paste(datapoints ," Most Common Trigrams"))
makePlot(freq2, paste(datapoints ," Most Common Bigrams"))
makePlot(freq1, paste(datapoints ," Most Common Unigrams"))
```


## Observations and Next Steps

This is a start on creating a predictive algorithm.  
